<html>
<head>
    <link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>
<h2 id="basicConcepts">Basic concepts of Data Processing Unit (DPU) creation </h2>
This section describes basic knowledge needed to create your own data processing unit (DPU) for UnifiedViews.

<h3>DPU</h3>
The data processing unit typically consists of three parts:

<ul>
    <li>main execution part, which contains the method <code>execute()</code>, which is called when the DPU is executed
        as a part of the pipeline execution.
        See the <a
                href="https://github.com/mff-uk/odcs/blob/master/module/SPARQL_transformer/src/main/java/cz/cuni/mff/xrg/odcs/transformer/SPARQL/SPARQLTransformer.java">SPARQL
            transformer</a> example.
    </li>
    <li>configuration object, which configures the main execution part and is used to persist the configuration dialog
        settings. See the <a
                href="https://github.com/mff-uk/odcs/blob/master/module/SPARQL_transformer/src/main/java/cz/cuni/mff/xrg/odcs/transformer/SPARQL/SPARQLTransformerConfig.java">SPARQL
            transformer</a> example.
    </li>
    <li>configuration dialog, which allows to configure the DPU in the graphical user interface of ODCleanStore. See the
        <a href="https://github.com/mff-uk/odcs/blob/master/module/SPARQL_transformer/src/main/java/cz/cuni/mff/xrg/odcs/transformer/SPARQL/SPARQLTransformerDialog.java">SPARQL
            transformer</a> example.
    </li>
</ul>
The DPU has to implement only the main executive part. The other parts are optional.
Based on the required features the DPU's programmer <a href="#whyUsePreImpl">should</a> use
an appropriate preimplemented class from the package <code>cz.cuni.mff.xrg.odcs.commons.module.dpu</code>:
<ul>
    <li><code>ConfigurableBase</code> class for DPU with configuration and optionally with a configuration dialog.</li>
    <li><code>NonConfigurableBase</code> class for DPU without configuration and dialog.</li>
</ul>
In order to provide a configuration dialog, the DPU's main class also has to implement an interface
<div class="code"> <pre> <code>
    interface cz.cuni.mff.xrg.odcs.commons.web.ConfigDialogProvider &lt;C extends DPUConfig&gt; </code> </pre>
</div>


<h3>Type of DPU</h3>

<div>The application distinguishes three types of DPUs:
    <ul>
        <li>Extractor - a DPU which extracts data from certain external data sources and produces outputs consumed by
            other DPUs.
        </li>
        <li>Transformer - a DPU which transforms locally available input data into output data.
            The difference between an extractor and a transformer is that the extractor downloads and triplifies
            external data, whereas the transformer works with data being already locally available for the ODCleanStore
            engine.
        </li>
        <li>Loader - a DPU which saves data into the target file, database, etc.
        </li>
    </ul>
    The DPU, you will be creating, must be annotated with one of the annotations <code>@AsExtractor, @AsTransformer,
        @AsLoader</code> to denote that the DPU is of one of the types above.
</div>


<h3 id="secConfigurationConcept">DPU Configuration</h3>

<div>A DPU has a possibility to store a configuration object that parametrizes its
    execution. The DPU developer may provide a configuration dialog for his DPU, so that the user may edit the
    configuration object via the
    dialog (the configuration object is synchronized with the dialog).
    <br/>

    The configuration object must implement an interface
    <div class="code"> <pre> <code>
        interface cz.cuni.mff.xrg.odcs.commons.configuration.DPUConfig </code> </pre>
    </div>
    which is located in the <code>commons</code> module. The interface requires implementation
    of one method - <code>isValid</code> - which checks whether the configuration is valid when the user's input is
    stored from the configuration dialog to the configuration object.
</div>
The configuration object may also be used to hold a default configuration of the DPU by introducing default values for
the required attributes.
<br/>Instead of implementing an interface the configuration class can extend
the following pre-implemented configuration class.
<div class="code"> <pre> <code>
    class cz.cuni.mff.xrg.odcs.commons.module.config.DPUConfig</code> </pre>
</div>
This class provides default 'always true' implementation of the <code>isValid</code> method,
this implementation can be optionally overridden by the programmer.
See the <a href="#whyUsePreImpl">reasons</a> why usage of the pre-implemented classes is a preferred solution.

<h3>DPU Template</h3>
Each DPU is associated with it's default (template) configuration (such configuration may contain, e.g., a file from
which data should be extracted or a SILK linker rule, which should be applied, etc.). A DPU together with it's default
(template) configuration is called a DPU template.


<h3>DPU Instance</h3>
When a DPU (e.g. RDF SPARQL Extractor, CSV Extractor, SILK Linker) is placed on the pipeline, such placement is called
DPU instantiation and the result of such instantiation is called DPU instance.
The DPU instance has its own configuration based on the template configuration of the DPU.
The DPU instance is always associated with the given pipeline.
One pipeline may have more different DPU instances of the same DPU template.

<h3>Data Units</h3>

<div>
    <p>Data between DPUs are passed in containers that are called DataUnits.
        Therefore, data units can be comprehended as data flow units, which describe data passed between the DPUs.
        DPUs distinguish input and output data units.
        Input data units are data units which flow into the processed DPU, output data units are those flowing out of
        the processed DPU.
        Currently only RDF data units (units carrying RDF data) are available.
    </p>

    <p>Data units cannot be created directly.
        To create a DataUnit, introduce a new <code>RDFDataUnit</code> <b>public</b> attribute of the created DPU and
        put the <code>@OutputDataUnit</code> annotation for output data unit and the <code>@InputDataUnit</code>
        annotation for input data unit. </p>

    <p>The following example demonstrate the functionality of adding new output data unit for the given DPU. By adding
        new data unit, you create new output data flow unit, which may be then filled with the data being outputted by
        that DPU. Every DPU may create one or more output data units.

    <div class="code"> <pre> <code>
        <span class="comment">// create output RDFDataUnit named "output"</span>
        @DataUnit.AsOutput(name="output")
        public RDFDataUnit outputDataUnit;
    </code> </pre>
    </div>
    </p>


    <p>The following example demonstrate the functionality of defining input data unit for DPU:

    <div class="code"> <pre> <code>
        <span class="comment">// prepare new input data unit </span><!--DataUnitList&lt;RDFDataUnit&gt; dataUnitList = RDFDataUnitList.create(context);-->
        @DataUnit.AsInput
        public RDFDataUnit inputDataUnit;
    </code> </pre>
    </div>
    </p>


    <code>RDFDataUnitHelper</code>
    provides methods for dealing with RDF content being passed in <code>RDFDataUnit</code> from one DPU to another DPU.
    The selected methods of <code>RDFDataUnitHelper</code> interface, which may be called on <code>RDFDataUnit</code>s
    by the DPU developer for processing content of data units or for filling the data units with new RDF data, are:
    <ul>
        <li> Methods for adding new RDF triples to Data Units
            <ul>
                <li>addTriple(Resource subject, URI predicate, Value object) - Add the defined triple (s,p,o) to the
                    DataUnit.
                </li>
                <li>addFromTurtleFile(File file) - Add triples from file to the data unit. The file type is Turtle
                    (TTL)
                </li>
                <li>addFromRDFXMLFile(File file) - Add triples from file to the data unit. The file type is RDF/XML</li>
                <li>addFromFile(File file, RDFFormat format) - Add triples from file to the data unit. The file type is
                    explicitely specified
                </li>

                <li>addFromSPARQLEndpoint(URL endpointURL, String namedGraph) - Add triples from SPARQL endpoint
                    specified by the endpoint URL and named graph to the data unit.
                </li>
                <li>addFromSPARQLEndpoint(URL endpointURL, String namedGraph,String user, String password) - Add triples
                    from SPARQL endpoint specified by the endpoint URL, named graph to the data unit, using user and
                    password provided.
                </li>

            </ul>
        </li>

        <li> Methods for retrieving RDF triples from Data Unit
            <ul>
                <li>List
                    <Statement> getTriples() - Get list of all RDF triples (Statements) in DataUnit.
                </li>
                <li>storeToFile(File file, RDFFormatType formatType) - Saves triples from data unit to file. The file
                    type is
                    explicitely specified.
                </li>
                <li>executeSelectQuery(String selectQuery) - Execute SPARQL select query over RDF data in DataUnit.</li>
                <li>executeConstructQuery(String selectQuery) - Execute SPARQL construct query over RDF data in
                    DataUnit.
                </li>

            </ul>
        </li>

        <li> Methods for transforming RDF data via queries
            <ul>
                <li>transform(String updateQuery) - Transform RDF data in DataUnit using SPARQL (update) query</li>
            </ul>
        </li>

    </ul>
    </li>


    </ul> For detailed information about the available methods please refer to the
    <span class="inCode">RDFDataUnit</span> class
</div>
</p>

<h3 id="context">Pipeline execution - DPU Context</h3>

<div>
    <p>Each DPU has its execution context. The main interface is
        <code>DPUContext</code>.

    <p> Context provides access to the external environment of the DPU, such as to the working directories of DPUs.
        There are three places where DPU can store it's temporary files needed for its execution.
    <ul>
        <li><b>working directory</b> - This directory is unique for single DPU instance (DPU placement on the pipeline)
            and the particular run of the pipeline. Only currently running DPUs can see such directory, which is private
            for the given DPU instance and pipeline run. Working directory may be obtained by calling <code>context.getWorkingDir();</code>
        </li>
        <li><b>user directory</b> - This directory is unique for every DPU and the user who executes the DPU. Therefore,
            you can store DPU's user related data here; such data is visible to all instances od that DPU executed by
            that user. Proper concurency mechanisms is needed, because user directory is shared. User directory may be
            obtained by calling <span class="code">context.getUserDirectory();</span></li>
        <li><b>global directory</b> - This directory is unique for a single DPU. It is
            shared by different DPU instances, pipeline runs, and users. You can store files that
            should be visible to your DPU at any time, such as certain caching mechanisms. Proper concurency mechanisms
            is needed, because global directory is shared. Global directory may be obtained by calling <code>context.getGlobalDirectory();</code>
        </li>
    </ul>

    <p>
        These directories can be obtained by calling specific context's methods. Alternatively, you can use <b>FileManager</b>
        helper class.
        Let's demonstrate it's functionality on few examples.The following example demonstrates how to get path to the
        file in
        the working directory with the use of File Manager:

    <div class="code"> <pre> <code>
        <span class="comment">// creates file manager</span>
        FileManager fileManager = new FileManager(context);
        <span class="comment">// obtains file in working directory</span>
        File filePath = fileManager.getWorking().file("myFile.txt");
    </code> </pre>
    </div>
    </p>

    <p>
        The following example demonstrates how to get the path to the file in
        the global sub-directory. The directory is automatically created.

    <div class="code"> <pre> <code>
        <span class="comment">// creates file manager</span>
        FileManager fileManager = new FileManager(context);
        <span class="comment">// obtains file in sub-directory in global directory</span>
        File filePath = fileManager.getGlobal().directory("myDir").file("myFile.txt");
    </code> </pre>
    </div>
    </p>

</div>


<p>The DPU context provides ability
    to send events of the DPU as it is executed. Such messages (events) are stored and accessible to the user via
    execution monitor in the administration interface of ODCleanStore.

<p>
    The following example demonstrates how the context may send various messages about the execution of the DPU.

<div class="code"> <pre> <code>
    <span class="comment">// sends error message</span>
    context.sendMessage(MessageType.ERROR, "MalformedURLException: "
    + ex.getMessage());
</code> </pre>
</div>
</p></li>

There are
two ways how to report that the execution of the DPU has failed:
<ol>
    <li>Send ERROR message via DPU context. In such case the execution
        normally continues and it is finished when <code>execute(DPUContext context)</code> ends.
        But no mather how the execution continues, the pipeline execution is consider to finished with error.
    <li>By throwing exception durring execution of - <code>execute(DPUContext context)</code> -.
        Best is to use <code>DPUException</code> with reasonable message that describes the reason
        of execution's failure.
    </li>
</ol>

Also in some cases the termination of the whole pipeline without failure can be required. As an example: we have
an extractor DPU that harvest new data from some webpage. If there are not new data, there is no meaning in further
pipeline processing, but this does not mean that the execution failed.<br/>
For such case the DPU can use <code>MessageType.TERMINATION_REQUEST</code> as message type. If this
message is send then the pipeline execution terminate after the current DPU. <br/>

If both ERROR and TERMINATION_REQUEST are send, then TERMINATION_REQUEST is ignored.

As the DPU's execution can be time consuming, ODCleanStore provides user with the possibility to
stop the execution. Unfortunately this can not be done without DPU's cooperation.
In order to cooperate, every DPU which should suppert its cancellation should periodically call <code>DPUContext.canceled()</code>
function. If this function returns true, the execution is stopped as soon as possible. If the
execution is stopped on the user's request then after the ends of <code>execute(DPUContext context)</code> -
function thee <code>cleanUp()</code> method is called. The <code>cleanUp()</code>
method should take care of releasing all resources allocated by the DPU.


<h3>Logging the DPU's activity</h3>

<div>Every DPU should log extensively during its execution. To log in your DPU, use <code>slf4j</code>.
    The log for each DPU is stored and accessible to the user by the
    graphical user interface of ODCleanStore.
    If you are not familiar with <code>slf4j</code>, the following example
    shows the basics:<br/>

    <div class="code"> <pre> <code>
        import org.slf4j.Logger;
        import org.slf4j.LoggerFactory;

        public class MyDPU {
        private final log = LoggerFactory.getLogger(MyDPU.class);
        ...
    </code> </pre>
    </div>

    <div class="code">
        <pre> <code>log.debug("Some message .. ");</code> <br/> <code>log.info("Some message .. ");</code> <br/> <code>log.warn("Some
            message .. ");</code> <br/> <code>log.error("Some message .. ");</code></pre>
    </div>
</div>

As you can see, there are various levels of logged messages. Basic log levels are DEBUG, INFO, WARNING, ERROR. If you
run pipeline, by default WARNING+ logs are shown in the execution monitor.
If you debug pipeline, all log levels are shown otherwise only INFO+ log are shown. If WARNING+ log is logged and
there is no error during whole pipeline execution the execution will finished with state FINISHED_WARNING.
In difference to the DPU's messages the execution can never failed because of logged log.

<h2 id="basicTechs">Basic Technologies</h2>

<h3 id="osgi">OSGi bundles</h3>

<p>The DPUs developed are provided to the ODCleanStore in the form of OSGi bundles. The OSGI can be view as the
    dependency management system.
    Bundle is a jar file with additional informations in MANIFEST.MF. The advantage of using OSGi bundles instead of
    simple JAR archives is that OSGi supports dynamic loadings of JAR files with their dependency resolving - OSGI uses
    information in jar's MANIFEST.MF to identify the dependencies of the bundle. Also every DPU may use its own set of
    dependencies (JAR libraries), which are hidden to other DPUs, thus, two DPUs may use two conflicting dependencies at
    once. </p>

<p> Each OSGi bundle contains <code>maninifest.mf</code> file located in the bundle, <code>META-INF</code> folder. This
    files contains bundle description. The most important part contains settings for exported and imported packages. The
    Export-Package is the list of packages provided by the bundle
    while the Import-Package is the list of packages that the bundle requires from
    others to export in order to work properly. Sample of this part is as follows:


<div class="maven-pom"> <pre> <code>
    Export-Package: cz.cuni.mff.xrg.odcs.loader.rdf
    Import-Package: com.vaadin.data,com.vaadin.data.util,com.vaadin.server,c
    om.vaadin.shared.ui.combobox,com.vaadin.ui,cz.cuni.mff.xrg.odcs.commons.c
    onfiguration;version="0.0.1",cz.cuni.mff.xrg.odcs.commons.data;version="0
    .0.1",cz.cuni.mff.xrg.odcs.commons.loader;version="0.0.1",cz.cuni.xrg.int
    lib.commons.module.data;version="0.0.1",cz.cuni.mff.xrg.odcs.commons.modu
    le.dialog;version="0.0.1",cz.cuni.mff.xrg.odcs.commons.module.dpu;version
    ="0.0.1",cz.cuni.mff.xrg.odcs.commons.web;version="0.0.1",cz.cuni.xrg.int
    lib.rdf.enums,cz.cuni.mff.xrg.odcs.rdf.exceptions,cz.cuni.mff.xrg.odcs.rdf.
    interfaces
</code> </pre>
</div>

<p> The <code>Export-Package</code> configuration option specifies packages that the DPU bundle provides to the external
    environment (ODCleanStore). On the other hand
    <code>Import-Package</code> configuration option lists dependencies of the bundle -- the packages that the bundle
    requires from
    others. As a result, such dependencies MUST be provide by the external environment (ODCleanStore) before the DPU is
    used. Dependencies are specified as packages, not JAR archives or classes in the <code>maninifest.mf</code> file.
</p>

Manifest file <code>maninifest.mf</code> also contains lines with <code>DPU-MainClass</code> and
<code>DPU-Package</code> that are essential for bundle use. <code>DPU-MainClass</code> contains name of the main class
of the DPU which is called when the DPU is executed on the pipeline. <code>DPU-Package</code> contains name of the
package in which the main class is located.

<h3>Maven</h3>

<p><a href="http://maven.apache.org/">Apache Maven</a> is a software project management and comprehension tool.
    The created DPUs are suggested to be maven projects, so that DPU bundles can be easily managed and created. In order
    to set up maven in your system, please follow <a href="http://maven.apache.org/run-maven/index.html">this site</a>
</p>

<p>TODO describe the minimum install - local repository </p>

<p>TODO describe the work with dependencies, scopes - compile, .. </p>


<p><code>Pom.xml</code> file is the basic configuration file needed when the maven project is being build. If you want
    to add dependency
    to the project (DPU bundle), <code>pom.xml</code> is the right place. See the sample <code>pom.xml</code> for <a
            href="https://github.com/mff-uk/odcs/blob/master/module/SPARQL_transformer/pom.xml">SPARQL transformer</a>
</p>

<h3 id="whyUsePreImpl">Why you should use preimplemented abstract classes and not interfaces, if these classes fit your
    needs?</h3>
The preimplemented classes contains default implementation for optionally implemented methods.
This is why their usage can made DPU's code simpler and also allows DPU's developer to focus
directly on implementation of DPU's executive functionality. <br/>
The other great advantage is easier switch to the newer ODCleanStore version. If a new
method will be introduced in the base interface then if possible default implementation
will be provided in respective preimplements class. Thanks to that the DPU's
code itself does not have to change. The DPU can be directly recompiled with
latest ODCleanStore version and it's readily to be used.

</body>
</html>